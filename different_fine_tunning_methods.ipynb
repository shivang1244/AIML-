{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate peft evaluate datasets sentencepiece bitsandbytes\n",
        "\n",
        "# 1) Imports\n",
        "import time\n",
        "import torch\n",
        "import pandas as pd\n",
        "from typing import List\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from peft import (\n",
        "    get_peft_model,\n",
        "    LoraConfig,\n",
        "    PrefixTuningConfig,\n",
        "    PromptTuningConfig,\n",
        "    TaskType,\n",
        ")\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "SGtYfCAs-rTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBLtkEZ7-rzU",
        "outputId": "a5be46b4-36da-41a1-b2cc-1e2755487de2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample = [\n",
        "    {\"text\": \"The Eiffel Tower is located in Paris and was constructed in 1889 for the World's Fair.\",\n",
        "     \"summary\": \"The Eiffel Tower in Paris was built in 1889 for the World's Fair.\"},\n",
        "    {\"text\": \"Apple announced a new iPhone with improved battery life and a more advanced camera system.\",\n",
        "     \"summary\": \"Apple announced a new iPhone with better battery life and an improved camera.\"},\n",
        "    {\"text\": \"Researchers discovered an exoplanet within the habitable zone roughly 200 light-years away.\",\n",
        "     \"summary\": \"Scientists discovered a potentially habitable exoplanet 200 light-years away.\"},\n",
        "    {\"text\": \"The government launched a plan to plant one million trees over five years to combat climate change.\",\n",
        "     \"summary\": \"A plan to plant one million trees in five years was launched to fight climate change.\"},\n",
        "    {\"text\": \"NASA launched a satellite to monitor global climate and collect atmospheric data.\",\n",
        "     \"summary\": \"NASA launched a climate-monitoring satellite to collect atmospheric data.\"},\n",
        "]\n",
        "texts = [x[\"text\"] for x in sample]\n",
        "refs  = [x[\"summary\"] for x in sample]"
      ],
      "metadata": {
        "id": "gackdRHz-xyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_IDS = {\n",
        "    \"flan-t5-small\": \"google/flan-t5-small\",\n",
        "    \"flan-t5-base\":  \"google/flan-t5-base\",\n",
        "    \"bart-base\":     \"facebook/bart-base\",\n",
        "}\n"
      ],
      "metadata": {
        "id": "rBIMMMIC-2O2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ensure_pad_token(tokenizer):\n",
        "    if tokenizer.pad_token is None:\n",
        "        if getattr(tokenizer, \"eos_token\", None) is not None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "        else:\n",
        "            tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
        "    return tokenizer\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_seq2seq(model, tokenizer, inputs: List[str], max_new_tokens: int = 64):\n",
        "    \"\"\"Generate outputs for seq2seq models. Returns list[str].\"\"\"\n",
        "    model.to(DEVICE).eval()\n",
        "    outs = []\n",
        "    for text in inputs:\n",
        "        enc = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=256).to(DEVICE)\n",
        "        gen = model.generate(**enc, max_new_tokens=max_new_tokens)\n",
        "        outs.append(tokenizer.decode(gen[0], skip_special_tokens=True))\n",
        "    return outs\n",
        "\n",
        "def timed_infer(fn, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    Run fn(*args, **kwargs) and measure elapsed time and GPU peak memory used (delta).\n",
        "    Returns (result, elapsed_seconds, peak_mem_bytes).\n",
        "    \"\"\"\n",
        "    # Ensure the helper itself is not shadowed anywhere else\n",
        "    if not callable(fn):\n",
        "        raise TypeError(\"First argument to timed_infer must be a callable function.\")\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "        start_mem = torch.cuda.memory_allocated()\n",
        "    else:\n",
        "        start_mem = 0\n",
        "    t0 = time.time()\n",
        "    result = fn(*args, **kwargs)\n",
        "    t1 = time.time()\n",
        "    peak = torch.cuda.max_memory_allocated() if torch.cuda.is_available() else 0\n",
        "    return result, (t1 - t0), max(0, peak - start_mem)\n",
        "\n",
        "# 6) Baseline inference\n",
        "print(\"\\n=== Baseline inference ===\")\n",
        "baseline = {}\n",
        "for name, mid in MODEL_IDS.items():\n",
        "    print(f\"\\nLoading {name} ({mid}) ...\")\n",
        "    tok = AutoTokenizer.from_pretrained(mid)\n",
        "    tok = ensure_pad_token(tok)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(mid).to(DEVICE)\n",
        "    # T5-style models benefit from task prefix\n",
        "    if \"flan-t5\" in mid or \"t5\" in mid:\n",
        "        inputs = [\"summarize: \" + t for t in texts]\n",
        "    else:\n",
        "        inputs = texts\n",
        "    outs, sec, mem = timed_infer(generate_seq2seq, model, tok, inputs, max_new_tokens=64)\n",
        "    baseline[name] = {\"outs\": outs, \"time\": sec, \"mem\": mem, \"tokenizer\": tok, \"model\": model}\n",
        "    print(f\"{name} first output:\\n\", outs[0])\n",
        "\n",
        "# 7) Prepare training tensors for flan-t5-small (tiny SFT)\n",
        "ft_model_id = MODEL_IDS[\"flan-t5-small\"]\n",
        "ft_tokenizer = AutoTokenizer.from_pretrained(ft_model_id)\n",
        "ft_tokenizer = ensure_pad_token(ft_tokenizer)\n",
        "train_inputs = [\"summarize: \" + t for t in texts]\n",
        "enc = ft_tokenizer(train_inputs, padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
        "with ft_tokenizer.as_target_tokenizer():\n",
        "    labels = ft_tokenizer(refs, padding=True, truncation=True, max_length=64, return_tensors=\"pt\")[\"input_ids\"]\n",
        "train_dataset = torch.utils.data.TensorDataset(enc[\"input_ids\"], enc[\"attention_mask\"], labels)\n",
        "\n",
        "def collate_seq2seq(batch):\n",
        "    input_ids = torch.stack([b[0] for b in batch])\n",
        "    attention_mask = torch.stack([b[1] for b in batch])\n",
        "    labels = torch.stack([b[2] for b in batch])\n",
        "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
        "\n",
        "# 8) Supervised Fine-Tuning (tiny demo: 1 epoch)\n",
        "print(\"\\n=== SFT: fine-tuning flan-t5-small (1 epoch, tiny demo) ===\")\n",
        "sft_model = AutoModelForSeq2SeqLM.from_pretrained(ft_model_id)\n",
        "sft_model.resize_token_embeddings(len(ft_tokenizer))\n",
        "sft_args = TrainingArguments(\n",
        "    output_dir=\"./sft-flan-small\",\n",
        "    per_device_train_batch_size=1,\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=5e-5,\n",
        "    logging_steps=1,\n",
        "    save_strategy=\"no\",\n",
        "    report_to=\"none\",\n",
        ")\n",
        "sft_trainer = Trainer(model=sft_model, args=sft_args, train_dataset=train_dataset, data_collator=collate_seq2seq)\n",
        "sft_trainer.train()\n",
        "\n",
        "# 9) PEFT demos on flan-t5-small: LoRA, Prompt; Prefix attempted with guard\n",
        "peft_results = {}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "YyVOFBwc-47f",
        "outputId": "aa8de591-892f-4a90-ce86-9ab312ca684b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Baseline inference ===\n",
            "\n",
            "Loading flan-t5-small (google/flan-t5-small) ...\n",
            "flan-t5-small first output:\n",
            " Located in Paris, the Eiffel Tower is the largest building in the world.\n",
            "\n",
            "Loading flan-t5-base (google/flan-t5-base) ...\n",
            "flan-t5-base first output:\n",
            " The Eiffel Tower is located in Paris.\n",
            "\n",
            "Loading bart-base (facebook/bart-base) ...\n",
            "bart-base first output:\n",
            " The Eiffel Tower is located in Paris and was constructed in 1889 for the World's Fair.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4006: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== SFT: fine-tuning flan-t5-small (1 epoch, tiny demo) ===\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5/5 00:00, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.539000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>10.339200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>7.553300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>5.608200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>12.212000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA\n",
        "print(\"\\n=== LoRA (PEFT) demo on flan-t5-small ===\")\n",
        "model_lora = AutoModelForSeq2SeqLM.from_pretrained(ft_model_id)\n",
        "lora_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, r=8, lora_alpha=16, lora_dropout=0.05)\n",
        "peft_lora = get_peft_model(model_lora, lora_config)\n",
        "peft_lora.resize_token_embeddings(len(ft_tokenizer))\n",
        "lora_args = TrainingArguments(\n",
        "    output_dir=\"./flan-lora\",\n",
        "    per_device_train_batch_size=1,\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=1e-4,\n",
        "    logging_steps=1,\n",
        "    save_strategy=\"no\",\n",
        "    report_to=\"none\",\n",
        ")\n",
        "lora_trainer = Trainer(model=peft_lora, args=lora_args, train_dataset=train_dataset, data_collator=collate_seq2seq)\n",
        "lora_trainer.train()\n",
        "peft_results[\"flan-lora\"] = peft_lora\n",
        "\n",
        "# Prompt-Tuning (stable)\n",
        "print(\"\\n=== Prompt-Tuning demo on flan-t5-small ===\")\n",
        "model_prompt = AutoModelForSeq2SeqLM.from_pretrained(ft_model_id)\n",
        "prompt_config = PromptTuningConfig(task_type=TaskType.SEQ_2_SEQ_LM, num_virtual_tokens=8)\n",
        "peft_prompt = get_peft_model(model_prompt, prompt_config)\n",
        "peft_prompt.resize_token_embeddings(len(ft_tokenizer))\n",
        "prompt_args = TrainingArguments(\n",
        "    output_dir=\"./flan-prompt\",\n",
        "    per_device_train_batch_size=1,\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=1e-4,\n",
        "    logging_steps=1,\n",
        "    save_strategy=\"no\",\n",
        "    report_to=\"none\",\n",
        ")\n",
        "prompt_trainer = Trainer(model=peft_prompt, args=prompt_args, train_dataset=train_dataset, data_collator=collate_seq2seq)\n",
        "prompt_trainer.train()\n",
        "peft_results[\"flan-prompt\"] = peft_prompt\n",
        "\n",
        "# Prefix-Tuning: conservative attempt with try/except\n",
        "print(\"\\n=== Prefix-Tuning attempt on flan-t5-small (guarded) ===\")\n",
        "prefix_success = False\n",
        "prefix_error = None\n",
        "try:\n",
        "    model_prefix = AutoModelForSeq2SeqLM.from_pretrained(ft_model_id)\n",
        "    # safe, small token count + projection; use model config for hidden dim\n",
        "    d_model = getattr(model_prefix.config, \"d_model\", None) or getattr(model_prefix.config, \"hidden_size\", None) or 512\n",
        "    prefix_config = PrefixTuningConfig(\n",
        "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "        num_virtual_tokens=4,\n",
        "        prefix_projection=True,\n",
        "        encoder_hidden_size=d_model,\n",
        "        decoder_hidden_size=d_model,\n",
        "    )\n",
        "    peft_prefix = get_peft_model(model_prefix, prefix_config)\n",
        "    peft_prefix.resize_token_embeddings(len(ft_tokenizer))\n",
        "    prefix_args = TrainingArguments(\n",
        "        output_dir=\"./flan-prefix\",\n",
        "        per_device_train_batch_size=1,\n",
        "        num_train_epochs=1,\n",
        "        learning_rate=1e-4,\n",
        "        logging_steps=1,\n",
        "        save_strategy=\"no\",\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "    prefix_trainer = Trainer(model=peft_prefix, args=prefix_args, train_dataset=train_dataset, data_collator=collate_seq2seq)\n",
        "    prefix_trainer.train()\n",
        "    peft_results[\"flan-prefix\"] = peft_prefix\n",
        "    prefix_success = True\n",
        "    print(\"Prefix-Tuning succeeded.\")\n",
        "except Exception as e:\n",
        "    prefix_success = False\n",
        "    prefix_error = str(e)\n",
        "    print(\"Prefix-Tuning failed (captured). Continuing. Error:\", prefix_error)\n",
        "\n",
        "# 10) QLoRA guard status (we won't run full QLoRA training here)\n",
        "try:\n",
        "    import bitsandbytes  # noqa: F401\n",
        "    qlora_status = \"bitsandbytes available; QLoRA possible but not run\"\n",
        "except Exception:\n",
        "    qlora_status = \"bitsandbytes not available\"\n",
        "print(\"\\nQLoRA status:\", qlora_status)\n",
        "\n",
        "# 11) Post-finetune inference and evaluation (ROUGE-L)\n",
        "print(\"\\n=== Post-finetune inference & evaluation ===\")\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "results = []\n",
        "\n",
        "# Baselines\n",
        "for name, info in baseline.items():\n",
        "    outs = info[\"outs\"]\n",
        "    sec = info[\"time\"]\n",
        "    mem = info[\"mem\"]\n",
        "    try:\n",
        "        r = rouge.compute(predictions=outs, references=refs)\n",
        "        rougeL = r.get(\"rougeL\")\n",
        "    except Exception:\n",
        "        rougeL = None\n",
        "    results.append({\n",
        "        \"Model\": name,\n",
        "        \"Setting\": \"baseline\",\n",
        "        \"ROUGE-L\": rougeL,\n",
        "        \"Inference time (s)\": round(sec, 4),\n",
        "        \"GPU mem delta (bytes)\": int(mem),\n",
        "        \"Model size after FT\": \"-\",\n",
        "        \"Notes\": \"baseline\"\n",
        "    })\n",
        "\n",
        "# SFT\n",
        "sft_outs, sft_time, sft_mem = timed_infer(generate_seq2seq, sft_model, ft_tokenizer, [\"summarize: \" + t for t in texts])\n",
        "try:\n",
        "    r = rouge.compute(predictions=sft_outs, references=refs)\n",
        "    rougeL = r.get(\"rougeL\")\n",
        "except Exception:\n",
        "    rougeL = None\n",
        "results.append({\n",
        "    \"Model\": \"flan-t5-small\",\n",
        "    \"Setting\": \"sft\",\n",
        "    \"ROUGE-L\": rougeL,\n",
        "    \"Inference time (s)\": round(sft_time, 4),\n",
        "    \"GPU mem delta (bytes)\": int(sft_mem),\n",
        "    \"Model size after FT\": \"full model (SFT)\",\n",
        "    \"Notes\": \"1 epoch, tiny\"\n",
        "})\n",
        "\n",
        "# PEFT models\n",
        "for tag, model in peft_results.items():\n",
        "    tok = ft_tokenizer\n",
        "    outs, ptime, pmem = timed_infer(generate_seq2seq, model, tok, [\"summarize: \" + t for t in texts])\n",
        "    try:\n",
        "        r = rouge.compute(predictions=outs, references=refs)\n",
        "        rougeL = r.get(\"rougeL\")\n",
        "    except Exception:\n",
        "        rougeL = None\n",
        "    results.append({\n",
        "        \"Model\": tag,\n",
        "        \"Setting\": \"peft\",\n",
        "        \"ROUGE-L\": rougeL,\n",
        "        \"Inference time (s)\": round(ptime, 4),\n",
        "        \"GPU mem delta (bytes)\": int(pmem),\n",
        "        \"Model size after FT\": \"adapter-only (PEFT)\",\n",
        "        \"Notes\": \"1 epoch, tiny\"\n",
        "    })\n",
        "    print(f\"\\n{tag} first output:\\n\", outs[0])\n",
        "\n",
        "# If prefix failed, record it\n",
        "if not prefix_success:\n",
        "    results.append({\n",
        "        \"Model\": \"flan-prefix\",\n",
        "        \"Setting\": \"prefix\",\n",
        "        \"ROUGE-L\": None,\n",
        "        \"Inference time (s)\": None,\n",
        "        \"GPU mem delta (bytes)\": None,\n",
        "        \"Model size after FT\": None,\n",
        "        \"Notes\": f\"prefix failed: {prefix_error}\"\n",
        "    })\n",
        "\n",
        "# QLoRA note\n",
        "results.append({\n",
        "    \"Model\": \"flan-qlora\",\n",
        "    \"Setting\": \"qlora\",\n",
        "    \"ROUGE-L\": None,\n",
        "    \"Inference time (s)\": None,\n",
        "    \"GPU mem delta (bytes)\": None,\n",
        "    \"Model size after FT\": \"adapter-only (4-bit base)\",\n",
        "    \"Notes\": qlora_status\n",
        "})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        },
        "id": "LkBLquuY--mj",
        "outputId": "78f67311-fa1e-4e30-b7e6-44d81abc3906"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== LoRA (PEFT) demo on flan-t5-small ===\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5/5 00:00, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.251100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>12.711400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>8.254700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>5.513200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>10.490200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Prompt-Tuning demo on flan-t5-small ===\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5/5 00:00, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.703200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>10.477100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>7.843300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>6.405800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>12.153200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Prefix-Tuning attempt on flan-t5-small (guarded) ===\n",
            "Prefix-Tuning failed (captured). Continuing. Error: PrefixTuningConfig.__init__() got an unexpected keyword argument 'decoder_hidden_size'\n",
            "\n",
            "QLoRA status: bitsandbytes available; QLoRA possible but not run\n",
            "\n",
            "=== Post-finetune inference & evaluation ===\n",
            "\n",
            "flan-lora first output:\n",
            " Located in Paris, the Eiffel Tower is the largest building in the world.\n",
            "\n",
            "flan-prompt first output:\n",
            " Eiffel Tower is located in Paris and was built in 1889 for the World's Fair\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(results)\n",
        "print(\"\\n=== Comparison table ===\")\n",
        "print(df)\n",
        "df.to_csv(\"comparison_table.csv\", index=False)\n",
        "print(\"\\nSaved comparison_table.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIIOebdj_L93",
        "outputId": "49304ec4-1241-46d0-ff19-d26e0883bebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Comparison table ===\n",
            "           Model   Setting   ROUGE-L  Inference time (s)  \\\n",
            "0  flan-t5-small  baseline  0.495685              1.9559   \n",
            "1   flan-t5-base  baseline  0.476018              4.8935   \n",
            "2      bart-base  baseline  0.705856              1.6391   \n",
            "3  flan-t5-small       sft  0.643135              1.2347   \n",
            "4      flan-lora      peft  0.495685              1.4967   \n",
            "5    flan-prompt      peft  0.614748              1.1678   \n",
            "6    flan-prefix    prefix       NaN                 NaN   \n",
            "7     flan-qlora     qlora       NaN                 NaN   \n",
            "\n",
            "   GPU mem delta (bytes)        Model size after FT  \\\n",
            "0              1547776.0                          -   \n",
            "1              3630592.0                          -   \n",
            "2             10306560.0                          -   \n",
            "3              1596928.0           full model (SFT)   \n",
            "4              1547776.0        adapter-only (PEFT)   \n",
            "5              1934336.0        adapter-only (PEFT)   \n",
            "6                    NaN                       None   \n",
            "7                    NaN  adapter-only (4-bit base)   \n",
            "\n",
            "                                               Notes  \n",
            "0                                           baseline  \n",
            "1                                           baseline  \n",
            "2                                           baseline  \n",
            "3                                      1 epoch, tiny  \n",
            "4                                      1 epoch, tiny  \n",
            "5                                      1 epoch, tiny  \n",
            "6  prefix failed: PrefixTuningConfig.__init__() g...  \n",
            "7  bitsandbytes available; QLoRA possible but not...  \n",
            "\n",
            "Saved comparison_table.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Example comparisons (first 3 samples) ===\")\n",
        "for i, text in enumerate(texts[:3]):\n",
        "    print(f\"\\nText [{i}]: {text}\\nReference: {refs[i]}\")\n",
        "    print(\"flan-t5-base (baseline):\", baseline[\"flan-t5-base\"][\"outs\"][i])\n",
        "    print(\"flan-t5-small (baseline):\", baseline[\"flan-t5-small\"][\"outs\"][i])\n",
        "    print(\"flan-t5-small (SFT):\", generate_seq2seq(sft_model, ft_tokenizer, [f\"summarize: {text}\"])[0])\n",
        "    print(\"flan-t5-small (LoRA):\", generate_seq2seq(peft_results[\"flan-lora\"], ft_tokenizer, [f\"summarize: {text}\"])[0])\n",
        "    print(\"flan-t5-small (Prompt-Tuning):\", generate_seq2seq(peft_results[\"flan-prompt\"], ft_tokenizer, [f\"summarize: {text}\"])[0])\n",
        "    if prefix_success:\n",
        "        print(\"flan-t5-small (Prefix-Tuning):\", generate_seq2seq(peft_results[\"flan-prefix\"], ft_tokenizer, [f\"summarize: {text}\"])[0])\n",
        "    else:\n",
        "        print(\"flan-t5-small (Prefix-Tuning): (failed — see table notes)\")\n",
        "\n",
        "print(\"\\nAll done.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sztwODCL_Zkh",
        "outputId": "a33e77e6-9a86-479f-8c09-8b551dc85678"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Example comparisons (first 3 samples) ===\n",
            "\n",
            "Text [0]: The Eiffel Tower is located in Paris and was constructed in 1889 for the World's Fair.\n",
            "Reference: The Eiffel Tower in Paris was built in 1889 for the World's Fair.\n",
            "flan-t5-base (baseline): The Eiffel Tower is located in Paris.\n",
            "flan-t5-small (baseline): Located in Paris, the Eiffel Tower is the largest building in the world.\n",
            "flan-t5-small (SFT): The Eiffel Tower was built in 1889 and was built in 1889.\n",
            "flan-t5-small (LoRA): Located in Paris, the Eiffel Tower is the largest building in the world.\n",
            "flan-t5-small (Prompt-Tuning): Eiffel Tower is located in Paris and was built in 1889 for the World's Fair\n",
            "flan-t5-small (Prefix-Tuning): (failed — see table notes)\n",
            "\n",
            "Text [1]: Apple announced a new iPhone with improved battery life and a more advanced camera system.\n",
            "Reference: Apple announced a new iPhone with better battery life and an improved camera.\n",
            "flan-t5-base (baseline): Apple announces new iPhone with improved battery life and more advanced camera system\n",
            "flan-t5-small (baseline): Apple announced a new iPhone with improved battery life and a more advanced camera system\n",
            "flan-t5-small (SFT): Apple announced a new iPhone with improved battery life and a more advanced camera system\n",
            "flan-t5-small (LoRA): Apple announced a new iPhone with improved battery life and a more advanced camera system\n",
            "flan-t5-small (Prompt-Tuning): Apple announces new iPhone with improved battery life and a more advanced camera system\n",
            "flan-t5-small (Prefix-Tuning): (failed — see table notes)\n",
            "\n",
            "Text [2]: Researchers discovered an exoplanet within the habitable zone roughly 200 light-years away.\n",
            "Reference: Scientists discovered a potentially habitable exoplanet 200 light-years away.\n",
            "flan-t5-base (baseline): An exoplanet has been discovered in the habitable zone.\n",
            "flan-t5-small (baseline): Scientists have discovered an exoplanet in the habitable zone around the planet Earth.\n",
            "flan-t5-small (SFT): Scientists discovered an exoplanet within the habitable zone roughly 200 light-years away.\n",
            "flan-t5-small (LoRA): Scientists have discovered an exoplanet in the habitable zone around the planet Earth.\n",
            "flan-t5-small (Prompt-Tuning): NASA scientists say they found an exoplanet within the habitable zone roughly 200 light-years away\n",
            "flan-t5-small (Prefix-Tuning): (failed — see table notes)\n",
            "\n",
            "All done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F91ewrQX_dSp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}